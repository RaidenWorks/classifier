{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The objective is to predict the survival of coronary artery disease patients using the dataset\n",
    "provided to help doctors to formulate preemptive medical treatments. In your submission, you\n",
    "are to evaluate at least 3 suitable models for estimating the patientsâ€™ survivals.\n",
    " \n",
    "## Dataset description\n",
    "The dataset contains the medical records of coronary artery disease patients for a particular\n",
    "hospital. Do note that there could be synthetic features in the dataset. Hence, please ensure\n",
    "that you state and verify any assumptions that you make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data\n",
    "2. Data exploration and cleaning\n",
    "   * General cleaning\n",
    "   * Analysis and re-formatting of data, identifying numerical vs categorical values\n",
    "   * Separate features and label\n",
    "   * Examine each feature's distribution for each label\n",
    "3. Training\n",
    "   * Split the data between train and test sets\n",
    "   * Train and Evaluate a Binary Classification Model\n",
    "4. Metrics Analysis\n",
    "   * Accuracy\n",
    "   * Recall\n",
    "   * Precision\n",
    "   * F1 Score\n",
    "   * Analysis and selection of metric\n",
    "   * Classification Report\n",
    "   * Confusion Matrix\n",
    "   * ROC Curve (Receiver Operator Characteristic)\n",
    "   * AUC (Area Under Curve)\n",
    "5. Pipeline creation\n",
    "   * Create a model\n",
    "   * Get prediction and plot performance\n",
    "6. Pipeline creation with refactored code\n",
    "   * Experimentation and analysis of features on predictive ability\n",
    "   * Experimentation and analysis of different algorithms on predictive ability\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQL data\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Get relative path\n",
    "dir = os.getcwd()\n",
    "fullpath = os.path.join(dir, 'data', 'survive.db') # join subdirectorys with ',' instead of '/' as need to be OS-independant\n",
    "print(fullpath)\n",
    "\n",
    "# Connection object\n",
    "o_conn = sqlite3.connect(fullpath)\n",
    "\n",
    "# Cursor object (for executing SQL queries against database)\n",
    "cur = o_conn.cursor()\n",
    "\n",
    "# List table names\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "s_tablename = 'survive' # change table name accordingly if not 'survive'\n",
    "s_SQLquery = 'SELECT * FROM ' + s_tablename\n",
    "\n",
    "df = pd.read_sql_query(s_SQLquery, o_conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Data exploration and cleaning\n",
    "\n",
    "### 2a. General cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty fields\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Creatinine records = 499\n",
    "# Or 499/15000 = 3.3% of records, made subjective call to drop them entirely.\n",
    "df = df.dropna(subset=['Creatinine'])\n",
    "\n",
    "# Verify they've been dropped\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick view of unique values for each column\n",
    "for column in df:\n",
    "    a = df[column].unique()\n",
    "    print(column, \":\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initial Observations*\n",
    "* **ID Unique** - ID for each patient\n",
    "* **Survive** - If the patient survives: 0 = No , 1 = Yes ***['0' '1' 'No' 'Yes']***\n",
    "* **Gender** - Gender type ***['Male' 'Female']***\n",
    "* **Smoke** - If the patient smokes ***['Yes' 'No' 'NO' 'YES']***\n",
    "* **Diabetes** - Diabetes conditions of patient ***['Normal' 'Pre-diabetes' 'Diabetes']***\n",
    "* **Age** - Age of the patient ***[integers, some negative]***\n",
    "* **Ejection Fraction** - Strength of heart ***['Low' 'Normal' 'High' 'L' 'N']***\n",
    "* **Sodium** - Level of sodium in the blood serum ( mg/dL) ***[integers]***\n",
    "* **Creatinine** - Level of creatinine in the blood serum (mEq/L) ***[floats]***\n",
    "* **Platelets** - Number of platelets in the blood serum (platelets/mL) ***[mostly integers in the thousands, some floats that look like errors]***\n",
    "* **Creatine phosphokinase** - Level of the enzyme in the blood (mcg/L) ***[floats]***\n",
    "* **Blood Pressure** - Level of blood pressure (mmHg) ***[integers]***\n",
    "* **Hemoglobin** - Level of hemoglobin in the blood (g/dL) ***[floats]***\n",
    "* **Height** - Height of patient (cm) ***[integers]***\n",
    "* **Weight** - Weight of patient (kg) ***[integers]***\n",
    "* **Favourite color** - Favourite color of patient ***['green' 'black' 'white' 'yellow' 'blue' 'red']***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in 'ID' to see if there are more than 1 each.\n",
    "print(df['ID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are duplicates. Check how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe recording counts of duplicates.\n",
    "df_ID = df['ID'].value_counts().reset_index()\n",
    "df_ID.columns = ['ID', 'count']\n",
    "\n",
    "# Then count those with more than 1 instance.\n",
    "print(len(df_ID[df_ID['count']>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 825 IDs with more than 1 entry. Need to investigate if they are duplicates or not. Check a sample entry.\n",
    "print(df[df['ID'] == 'FHKHIH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 4 different individuals, we can't drop based on duplicate ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop_duplicates(subset='ID', keep='first') # This keeps the first entry and drops the others based on ID alone. We can't do this as established above.\n",
    "\n",
    "# Combine the column values\n",
    "df['combined'] = ['_'.join(row.astype(str)) for row in df[df.columns[1:]].values]\n",
    "print(df['combined'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop_duplicates(['combined']) # We don't need to drop any duplicates as there are none.\n",
    "\n",
    "df.drop('combined', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2b. Analysis and re-formatting of data, identifying numerical vs categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Survive'].unique())\n",
    "print(df['Survive'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform strings to 0 and 1\n",
    "df = df.replace({'Survive' : { '0' : 0, '1' : 1, 'No' : 0, 'Yes' : 1}})\n",
    "print(df['Survive'].unique())\n",
    "print(df['Survive'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Survive' will be the label in the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Gender'].unique())\n",
    "print(df['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform strings to 0 and 1\n",
    "df = df.replace({'Gender' : { 'Male' : 1, 'Female' : 0}})\n",
    "print(df['Gender'].unique())\n",
    "print(df['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Gender' being a categorical feature, will be One Hot Encoded later in the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Smoke'].unique())\n",
    "print(df['Smoke'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform strings to 0 and 1\n",
    "df = df.replace({'Smoke' : { 'Yes' : 1, 'No' : 0, 'NO' : 0, 'YES' : 1}})\n",
    "print(df['Smoke'].unique())\n",
    "print(df['Smoke'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Smoke' being a categorical feature, will be One Hot Encoded later in the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Diabetes'].unique())\n",
    "print(df['Diabetes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'Diabetes' : { 'Normal' : 0, 'Pre-diabetes' : 1, 'Diabetes' : 2}})\n",
    "print(df['Diabetes'].unique())\n",
    "print(df['Diabetes'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Diabetes' being a categorical feature, will be One Hot Encoded later in the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Age'].unique())\n",
    "print(df['Age'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].abs() # assume that recorded negative values are meant to positive\n",
    "print(df['Age'].unique())\n",
    "print(df['Age'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution to have a view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Age' distribution\n",
    "# plotter.histo_boxplot(df, 'Age', 20)\n",
    "distri_age = plotter.Distribution(df, 'Age', 20)\n",
    "distri_age.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Age' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Ejection Fraction'].unique())\n",
    "print(df['Ejection Fraction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'Ejection Fraction' : { 'Low' : 0, 'Normal' : 1, 'High' : 2, 'L' : 0, 'N' : 1}})\n",
    "print(df['Ejection Fraction'].unique())\n",
    "print(df['Ejection Fraction'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Ejection Fraction' being a categorical feature, will be One Hot Encoded later in the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Sodium'].unique())\n",
    "print(df['Sodium'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Sodium' distribution\n",
    "# plotter.histo_boxplot(df, 'Sodium', 20)\n",
    "distri_sodium = plotter.Distribution(df, 'Sodium', 20)\n",
    "distri_sodium.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhbits central tendency. 'Sodium' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Creatinine'].unique())\n",
    "print(df['Creatinine'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Creatinine' distribution\n",
    "# plotter.histo_boxplot(df, 'Creatinine', 40)\n",
    "distri_creatinine = plotter.Distribution(df, 'Creatinine', 40)\n",
    "distri_creatinine.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive skew distribution. 'Creatinine' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Platelets'].unique())\n",
    "print(df['Platelets'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Platelets' distribution\n",
    "# plotter.histo_boxplot(df, 'Platelets', 50)\n",
    "distri_platelets = plotter.Distribution(df, 'Platelets', 50)\n",
    "distri_platelets.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal-looking distribution. However, there's an strange 263358.03 value. Check how many records might have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in 'Platelets' to see if there are more than 1 each.\n",
    "df['Platelets'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entries with the same 263358.03 are unusually high. This appears to be an errorneous entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_StrangePlatelets = len(df[df['Platelets']==263358.03]) # number of records with Platelets as 263358.03\n",
    "num_Records = len(df) # total remaining valid records\n",
    "pc_StrangePlatelets = num_StrangePlatelets / num_Records\n",
    "print(pc_StrangePlatelets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.35% of records have the same strange platelet count of 236658.03. That's quite large.\n",
    "\n",
    "Have to decide whether to drop them or round to the nearest thousand to match the format of the rest. Or change to mean value.\n",
    "\n",
    "It's a very specific error though. If Platelet count is a major factor in prediction, this error will greatly affect the result.\n",
    "\n",
    "Have an overview of the records with this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Platelets'] == 263358.03])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row looks different enough for the same strange Platelet count to be equal. Even though it's 8.35% of the data set, decided to drop these rows as it seems the entry is from human error and the potential effect on prediction is costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows1 = len(df.index)\n",
    "print('rows before deletion:',rows1)\n",
    "\n",
    "# drop rows where 'Platelets' == 263358.03\n",
    "df = df[df.Platelets != 263358.03] # this is a very specific condition, could consider deleting values not rounded to thousands *000s\n",
    "\n",
    "rows2 = len(df.index)\n",
    "print('rows after deletion:',rows2)\n",
    "\n",
    "print('rows deleted:',rows1 - rows2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-plot 'Platelets' distribution\n",
    "# plotter.histo_boxplot(df, 'Platelets', 50)\n",
    "distri_platelets = plotter.Distribution(df, 'Platelets', 50)\n",
    "distri_platelets.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to adversely affect the distribution.\n",
    "'Platelets' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Creatine phosphokinase'].unique())\n",
    "print(df['Creatine phosphokinase'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Creatine phosphokinase' distribution\n",
    "# plotter.histo_boxplot(df, 'Creatine phosphokinase', 40)\n",
    "distri_cphospho = plotter.Distribution(df, 'Creatine phosphokinase', 40)\n",
    "distri_cphospho.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily positively skewed. 'Creatine phosphokinase' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Blood Pressure'].unique())\n",
    "print(df['Blood Pressure'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Blood Pressure' distribution\n",
    "# plotter.histo_boxplot(df, 'Blood Pressure', 120)\n",
    "distri_bp = plotter.Distribution(df, 'Blood Pressure', 120)\n",
    "distri_bp.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not exhibit central tendency. Certain values separated by a somewhat constant distance appear at twice the frequency as the rest of the values.\n",
    "\n",
    "It might be possible that this is aggregated data from multiple sources and one source is rounding values according to the discrete jumps.\n",
    "\n",
    "'Blood Pressure' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Hemoglobin'].unique())\n",
    "print(df['Hemoglobin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Hemoglobin' distribution\n",
    "# plotter.histo_boxplot(df, 'Hemoglobin', 40)\n",
    "distri_haemo = plotter.Distribution(df, 'Hemoglobin', 40)\n",
    "distri_haemo.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not exhibit central tendency. 'Hemoglobin' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Height'].unique())\n",
    "print(df['Height'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Height' distribution\n",
    "# plotter.histo_boxplot(df, 'Height', 50)\n",
    "distri_height = plotter.Distribution(df, 'Height', 50)\n",
    "distri_height.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite evenly spread out but again we see certain values separated by a somewhat constant distance appearing at twice the frequency as the rest of the values.\n",
    "It does seem likely this is aggregated data from multiple sources and one source is rounding values according to the discrete jumps. It remains to be seen whether 'Height' is a good predictor of survivability, but if the later tests show it is, this should be investigated further. Possibly by separating this data apart and retesting.\n",
    "Would have expected a normal-looking distribution for this feature though.\n",
    "'Height' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Weight'].unique())\n",
    "print(df['Weight'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Weight' distribution\n",
    "# plotter.histo_boxplot(df, 'Weight', 120)\n",
    "distri_weight = plotter.Distribution(df, 'Weight', 120)\n",
    "distri_weight.histo_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhibits central tendency. 'Weight' being a numeric feature, will be scaled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Favorite color'].unique())\n",
    "print(df['Favorite color'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'Favorite color' : { 'green' : 0, 'black' : 1, 'white' : 2, 'yellow' : 3, 'blue' : 4, 'red' : 5}})\n",
    "print(df['Favorite color'].unique())\n",
    "print(df['Favorite color'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Favorite color' being a categorical feature, will be One Hot Encoded later in the classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of changes\n",
    "for column in df:\n",
    "    a = df[column].unique()\n",
    "    print(column, \":\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyse the size of each label.\n",
    "\n",
    "We will look to predict the label **'Survive'** = **0** for patients who did not survived coronary artery disease, and **1** for patients who did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly we want to see that size of each label is significant, otherwise the Accuracy metric might not be meaningful.\n",
    "num_survivors = len(df[df['Survive'] == 1])\n",
    "num_nonsurvivors = len(df[df['Survive'] == 0])\n",
    "\n",
    "print('Number of survivors:', num_survivors)\n",
    "print('Number of non-survivors:', num_nonsurvivors)\n",
    "rows = len(df)\n",
    "print('Percentage of survivors: {:.1f}'.format(num_survivors/rows*100),\"%\")\n",
    "print('Percentage of non-survivors: {:.1f}'.format(num_nonsurvivors/rows*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of each label is significant but not symmetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of reformatted data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2c. Separate features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste to features below\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "label = 'Survive'\n",
    "X, y = df[features].values, df[label].values\n",
    "\n",
    "for n in range(0,4):\n",
    "    print(\"Patient\", str(n+1), \"\\n  Features:\",list(X[n]), \"\\n  Label:\", y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2d. Examine each feature's distribution for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "for col in features:\n",
    "    df.boxplot(column=col, by='Survive', figsize=(6,6))\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight** shows markedly different distributions for non-survivors and survivors.\n",
    "\n",
    "**Hemoglobin**, **Blood Pressure**, **Creatinine**, **Sodium**, **Age** distributions also show some difference.\n",
    "\n",
    "***\n",
    "\n",
    "# 3. Training\n",
    "\n",
    "### 3a. Split the data between train and test sets\n",
    "\n",
    "We now need to train a classifier so that it finds a relationship between the features and the label values.\n",
    "\n",
    "In order to test the effectiveness of our training, we need to reserve a portion of the data for testing, so that we can compare the predicted labels with the already known labels of the test set.\n",
    "\n",
    "We use the **train_test_split** function from **scikit-learn** package to get a statistically random split of training and test data. We will reserve 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "split_testsize = 0.30\n",
    "split_seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_testsize, random_state=split_seed)\n",
    "\n",
    "print ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3b. Train and Evaluate a Binary Classification Model\n",
    "We now train our model by fitting the *training features* (**X_train**) to the *training labels* (**y_train**). We'll start with using the *Logistic Regression*, an algorithm for classification.\n",
    "\n",
    "We also set a *regularization* parameter, which is used to counteract any bias in the sample, and help the model generalize by avoiding *overfitting* the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "LogReg_RegularizationRate = 0.1\n",
    "\n",
    "model = LogisticRegression(C=1/LogReg_RegularizationRate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained using the training data.\n",
    "\n",
    "We now use the test data earlier held back to evaluate how well it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict labels for the test set, and compare the predicted labels to the known labels.\n",
    "predictions = model.predict(X_test)\n",
    "print('Predicted labels:', predictions)\n",
    "print('Actual labels:   ', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4. Metrics Analysis\n",
    "\n",
    "There's too much to ingest visually so we'll examine some metrics with the below:\n",
    "* Accuracy\n",
    "* Recall\n",
    "* Precision\n",
    "* F1 Score\n",
    "* Classification Report\n",
    "* Confusion Matrix\n",
    "* ROC Curve (Receiver Operator Characteristic)\n",
    "* AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some definitions first:\n",
    "* *True Positives* (TP): The predicted label and the actual label are both 1.\n",
    "* *False Positives* (FP): The predicted label is 1, but the actual label is 0.\n",
    "* *False Negatives* (FN): The predicted label is 0, but the actual label is 1.\n",
    "* *True Negatives* (TN): The predicted label and the actual label are both 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4a. Accuracy\n",
    "* The ratio of *correct* predictions to the *total* number of predictions.\n",
    "* Accuracy = (TP+TN)/(TP+TN+FP+FN) = (sum of True Positive and True Negative predictions) / (total number of predictions)\n",
    "* Answers this question: ***Of all predictions, how many are predicted correctly?***\n",
    "* Value of 1 means 100% of the model's predictions were right, with 0 meaning 100% were wrong.\n",
    "* It is a good measure if the labels are generally symmetrical, inversely, if the percentage of non-survivors is too small, e.g. 5%, a classifier that always predicts 0 cases of non-survivors will be 95% accurate, i.e. the accuracy of 95% is meaningless.\n",
    "* Accuracy might not be the best metric as the population of non-survivors(67.6%) is twice that of survivors(32.4%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sk_accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy:', sk_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is 79.05% accurate. Quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4b. Recall\n",
    "* The ratio of *correct positive* predictions to the *total correct* predictions.\n",
    "* Recall = TP/(TP+FN) = (True Positive predictions) / (sum of True Positive and False Negative predictions)\n",
    "* Answers this question: ***Of all that are actually positive, how many are predicted positive?***\n",
    "* Recall is particularly useful in cases where the impact of a False Negative is severe, e.g. classifying whether you've been hacked or not, classifying whether an elevator cable is service-worthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sk_recall = recall_score(y_test, predictions)\n",
    "print('Recall:', sk_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recall is low. Of the total predictions that are actually 1 (Survived), it only predicted 55.3 % as being 1 (Survived)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4c. Precision\n",
    "* The ratio of *correct positive* predictions to the *total positive* predictions.\n",
    "* Precision = TP/(TP+FP) = (True Positive predictions) / (sum of True Positives and False Positives)\n",
    "* Answers this question: ***Of all that have been predicted positive, how many are actually positive?***\n",
    "* A high Precision equates to a low False Positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "sk_precision = precision_score(y_test, predictions)\n",
    "print('Precision:', sk_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is 73.0% precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4d. F1 Score\n",
    "* The weighted average of Recall and Precision.\n",
    "* F1 Score = 2 * (Recall * Precision) / (Recall + Precision)\n",
    "* F1 is usually more useful than Accuracy, particularly with an uneven label distribution.\n",
    "* Accuracy works best if false positives and false negatives have similar cost. If they are very different, it is better to look at Recall, Precision, and by extension, F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sk_f1score = f1_score(y_test, predictions) # (2 * sk_recall * sk_precision)/(sk_recall + sk_precision)\n",
    "print('F1 Score:', sk_f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4e. Analysis and selection of metric\n",
    "* Accuracy - we've seen that the populations of non-survivors and survivors are not entirely symmetrical\n",
    "* Recall = (True Positive predictions) / (sum of True Positive and False Negative predictions)\n",
    "* Precision = (True Positive predictions) / (sum of True Positives and False Positives)\n",
    "\n",
    "**If this model is to be used as an *initial diagnosis***:\n",
    "\n",
    "* We want a *high Recall* (patients who actually are at risk) so that we can send them for further tests.\n",
    "* We can accept a *lower Precision* if the costs of further tests are not significant.\n",
    "* We choose **Recall** in this scenario.\n",
    "\n",
    "**If this model is to be used to decide whether to send the patient for *surgery***:\n",
    "\n",
    "* We similarly want a *high Recall*, as the cost of a false negative is high (patient does not get life-saving surgery).\n",
    "* However, we also want *high Precision* as we do not want to send patients for unnecessary surgery. The cost of a false positive is similarly high.\n",
    "* We choose **F1 Score** in this scenario.\n",
    "\n",
    "As stated in the objectives, since we are targeting to help doctors formulate preemptive medical treatments, we can accept a lower **Precision**, however, we'll still want a high **Recall**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4f. Classification Report\n",
    "\n",
    "This convenient report includes the above metrics for each class of the label (0 and 1), and also\n",
    "\n",
    "* *Support*: How many instances of this class are there in the test dataset?\n",
    "\n",
    "The classification report also includes averages for these metrics, including a weighted average that allows for the imbalance in the number of cases of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4g. Confusion Matrix\n",
    "\n",
    "View the confusion matrix for an overview of the prediction distribution.\n",
    "\n",
    "Note that the correct (*true*) predictions form a diagonal line from top left to bottom right - these figures should be significantly higher than the *false* predictions if the model is any good.\n",
    "\n",
    "It provides a good snapshot overview of the predictions in relation to the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TN | FP\n",
    "# ---+---\n",
    "# FN | TP\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TP = cm[1][1]\n",
    "print('TN:',TN)\n",
    "print('FP:',FP)\n",
    "print('FN:',FN)\n",
    "print('TP:',TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition a confusion matrix C is such that C[i, j] is equal to the number of observations known to be in group i but predicted to be in group j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4h. ROC Curve (Receiver Operator Characteristic)\n",
    "\n",
    "So far, we've considered the predictions from the model as being either 0 or 1 class labels.\n",
    "\n",
    "What actually gets predicted by a binary classifier is the probability that the label is true (**P(y)**) and the probability that the label is false (1 - **P(y)**).\n",
    "\n",
    "A threshold value of 0.5 is used to decide whether the predicted label is a 1 (*P(y) > 0.5*) or a 0 (*P(y) <= 0.5*). You can use the **predict_proba** method to see the probability pairs for each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = model.predict_proba(X_test)\n",
    "print(y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilities are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the *true positive rate* (which is another name for recall) and the *false positive rate* for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a ***Receiver Operator Characteristic (ROC) chart***, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction.\n",
    "***\n",
    "### 4i. AUC (Area Under Curve)\n",
    "The area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC at 0.8319 is moderately good.\n",
    "***\n",
    "# 5. Pipeline creation\n",
    "\n",
    "We now create a pipeline that does some more preprocessing of the data to make it better for the algorithm to fit a model to it. We'll use 2 here:\n",
    "\n",
    "- Scaling numeric features so they're on the same scale. This prevents features with large values from producing coefficients that disproportionately affect the predictions.\n",
    "- Encoding categorical variables. By using a *one hot encoding* technique we can create individual binary (true/false) features for each possible category value.\n",
    "\n",
    "We then select an algorithm to train a model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - Gender\n",
    "# 1 - Smoke\n",
    "# 2 - Diabetes\n",
    "# 3 - Age\n",
    "# 4 - Ejection Fraction\n",
    "# 5 - Sodium\n",
    "# 6 - Creatinine\n",
    "# 7 - Platelets\n",
    "# 8 - Creatine phosphokinase\n",
    "# 9 - Blood Pressure\n",
    "# 10 - Hemoglobin\n",
    "# 11 - Height\n",
    "# 12 - Weight\n",
    "# 13 - Favorite color\n",
    "\n",
    "# Numeric features -\n",
    "# Age, Sodium, Creatinine, Platelets, Creatine phosphokinase, Blood Pressure, Hemoglobin, Height, Weight\n",
    "# [3,5,6,7,8,9,10,11,12]\n",
    "\n",
    "# Categorical features -\n",
    "# Gender, Smoke, Diabetes, Ejection Fraction, Favorite color\n",
    "# [0,1,2,4,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5a. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define preprocessing for numeric columns (make them on the same scale)\n",
    "numeric_features = [3,5,6,7,8,9,10,11,12]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = [0,1,2,4,13]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', LogisticRegression(C=1/LogReg_RegularizationRate, solver=\"liblinear\"))])\n",
    "\n",
    "# fit the pipeline to train a logistic regression model on the training set\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose of this cell is to generate the print output of the selected features for the 1st subplot in the charts further below\n",
    "\n",
    "# Dictionary for returning the string of the features\n",
    "features_dict = {\n",
    "    0: 'Gender',\n",
    "    1: 'Smoke',\n",
    "    2: 'Diabetes',\n",
    "    3: 'Age',\n",
    "    4: 'Ejection Fraction',\n",
    "    5: 'Sodium',\n",
    "    6: 'Creatinine',\n",
    "    7: 'Platelets',\n",
    "    8: 'Creatine phosphokinase',\n",
    "    9: 'Blood Pressure',\n",
    "    10: 'Hemoglobin',\n",
    "    11: 'Height',\n",
    "    12: 'Weight',\n",
    "    13: 'Favorite color'\n",
    "}\n",
    "\n",
    "# Initialise empty string list for feature names\n",
    "features_list = []\n",
    "\n",
    "# Combine int list of categorical and numeric features\n",
    "# categorical_features = [0, 1, 2, 4, 13]\n",
    "# numeric_features = [3, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "# combined_features = [0, 1, 2, 4, 13, 3, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "combined_features = categorical_features + numeric_features\n",
    "\n",
    "# Generate string list from the int list referencing the dictionary\n",
    "for item in combined_features:\n",
    "    features_list.append(features_dict[item])\n",
    "\n",
    "# Generate a continuous string with each string feature on a new line\n",
    "features_down = '\\n'.join(features_list)\n",
    "print(features_down)\n",
    "# print('\\n'.join(feature_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5b. Get prediction and plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get predictions from test data\n",
    "predictions = model.predict(X_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# Format plots arrangement\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(24, 6))\n",
    "\n",
    "# Print label and features in 1st subplot\n",
    "fontsize = 14\n",
    "padding_left_Attrib = 0.1\n",
    "padding_left_Value = 0.40\n",
    "padding_top = 0.96\n",
    "padding_top_increment = 0.07\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].text(padding_left_Attrib, padding_top, 'Algorithm:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[0].text(padding_left_Value, padding_top, 'Logistic Regression', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[0].text(padding_left_Attrib, padding_top-padding_top_increment, 'Test size:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[0].text(padding_left_Value, padding_top-padding_top_increment, '{:.2f}'.format(split_testsize), size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[0].text(padding_left_Attrib, padding_top-2*padding_top_increment, 'Seed:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[0].text(padding_left_Value, padding_top-2*padding_top_increment, str(split_seed), size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[0].text(padding_left_Attrib, padding_top-3*padding_top_increment, 'Label:', size=fontsize, va='top', ha='left')\n",
    "ax[0].text(padding_left_Value, padding_top-3*padding_top_increment, 'Survive', size=fontsize, va='top', ha='left')\n",
    "ax[0].text(padding_left_Attrib, padding_top-4*padding_top_increment, 'Features:', size=fontsize, va='top', ha='left')\n",
    "ax[0].text(padding_left_Value, padding_top-4*padding_top_increment, features_down, size=fontsize, va='top', ha='left')\n",
    "\n",
    "# Plot ROC curve in 2nd subplot\n",
    "ax[1].set_aspect('equal', adjustable='box')\n",
    "ax[1].plot([0, 1], [0, 1], 'k--')\n",
    "ax[1].plot(fpr, tpr)\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('ROC Curve')\n",
    "\n",
    "# Plot Confusion Matrix in 3rd subplot\n",
    "import seaborn as sns\n",
    "cf_matrix = confusion_matrix(y_test, predictions)\n",
    "group_names = ['True non-survivor','False survivor','False non-survivor','True survivor']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "hmap = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', ax=ax[2], square=True)\n",
    "hmap.set_title('Confusion Matrix\\n')\n",
    "hmap.set_xlabel('Predicted Values')\n",
    "hmap.set_ylabel('Actual Values')\n",
    "hmap.xaxis.set_ticklabels(['non-survivor','survivor'])\n",
    "hmap.yaxis.set_ticklabels(['non-survivor','survivor'])\n",
    "\n",
    "# Plot metrics in 4th subplot\n",
    "f_accuracy_logres = accuracy_score(y_test, predictions)\n",
    "f_recall_logres = recall_score(y_test, predictions)\n",
    "f_precision_logres = precision_score(y_test, predictions)\n",
    "f_f1score = f1_score(y_test, predictions)\n",
    "f_auc = roc_auc_score(y_test, y_scores[:,1])\n",
    "s_auc = '{:.4f}'.format(f_auc) # save the string to be used as part of the JPG name\n",
    "fontsize = 14\n",
    "padding_left_Attrib = 0.15\n",
    "padding_left_Value = 0.45\n",
    "padding_top = 0.66\n",
    "padding_top_increment = 0.07\n",
    "ax[3].axis(\"off\")\n",
    "ax[3].text(padding_left_Attrib, padding_top, 'Accuracy:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Value, padding_top, '{:.4f}'.format(f_accuracy_logres), size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Attrib, padding_top-padding_top_increment, 'Recall:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Value, padding_top-padding_top_increment, '{:.4f}'.format(f_recall_logres), size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Attrib, padding_top-2*padding_top_increment, 'Precision:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Value, padding_top-2*padding_top_increment, '{:.4f}'.format(f_precision_logres), size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Attrib, padding_top-3*padding_top_increment, 'F1 Score:', size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Value, padding_top-3*padding_top_increment, '{:.4f}'.format(f_f1score), size=fontsize, va=\"top\", ha=\"left\")\n",
    "ax[3].text(padding_left_Attrib, padding_top-4*padding_top_increment, 'AUC:', size=fontsize, va='top', ha='left')\n",
    "ax[3].text(padding_left_Value, padding_top-4*padding_top_increment, s_auc, size=fontsize, va='top', ha='left')\n",
    "\n",
    "# Save plot as JPG file in output folder\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "# dt_string = datetime.now().strftime(\"%Y-%m-%d %H%M%S\") # for prefixing datetime to file name\n",
    "# filename = dt_string+' '+'Logistic Regresion'+', AUC - '+s_auc+'.jpg' # AUC metric tag serves as a quick comparison between runs while seeing in the folder\n",
    "# parentdirectory = os.path.abspath(os.getcwd()) # gets directory this script is run in\n",
    "# outputdirectory = os.path.join(parentdirectory, 'output') # place JPGs in output folder one directory above where script is\n",
    "# fig.savefig(os.path.join(outputdirectory, filename), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing (scaling and one hot encoding) has improved the model's prediction.\n",
    "\n",
    "Accuracy: 0.7906 -> 0.8387\n",
    "\n",
    "Recall: 0.5530 -> 0.6841\n",
    "\n",
    "Precision 0.7302 -> 0.7865\n",
    "\n",
    "F1 Score: 0.6294 -> 0.7317\n",
    "\n",
    "AUC: 0.8319 -> 0.8731\n",
    "***\n",
    "# 6. Pipeline creation with refactored code\n",
    "We can test individual features to see how they singly contribute to the predictions, and also see the performance of other classification algorithms.\n",
    "\n",
    "We will use re-factored code from the 'src' subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline_classifier import *\n",
    "\n",
    "i_algo = 1\n",
    "# Enter integer 1 to 4 to select the ML algorithm to train and test the model on.\n",
    "# 1 - Logistic Regression\n",
    "# 2 - Random Forest Classifier\n",
    "# 3 - Support Vector Machine\n",
    "# 4 - K-Nearest Neighbour\n",
    "\n",
    "f_testsize = 0.3\n",
    "# Enter a float to set the train-test size\n",
    "# e.g. 0.3 -> 30% of data is reserved for testing\n",
    "\n",
    "i_seed = 42\n",
    "# Enter the seed for the random state shuffling during train-test data split (useful for reproducibility).\n",
    "# Set to <None> to allow randomisation across different runs.\n",
    "\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "# Enter the features you want as a list.\n",
    "# ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6a. Experimentation and analysis of features on predictive ability\n",
    "Let's examine the impact of individual features on the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_algo = 1\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "\n",
    "features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "for x in features:\n",
    "    ls_features = [x]\n",
    "    fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# ls_features = ['Gender']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Smoke']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Diabetes']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Age']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Ejection Fraction']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Sodium']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Creatinine']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Platelets']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Creatine phosphokinase']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Blood Pressure']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Hemoglobin']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Height']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Weight']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)\n",
    "# ls_features = ['Favorite color']\n",
    "# fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation on the predictive ability of the Logistic Regression classification model just run:\n",
    "\n",
    "Gender, Smoke, Diabetes, Creatine phosphokinase, Height and Favorite Color contribute the least to the model's predictive ability.\n",
    "\n",
    "Ejection Fraction, Platelets, Blood Pressure, and Hemoglobin contribute a little.\n",
    "\n",
    "While Age, Sodium, Creatinine, and Weight have the largest effect on predictive ability.\n",
    "\n",
    "We also evidently see that a high Precision does not mean much if the model predicts everyone as survivors.\n",
    "\n",
    "Intuitively, we would expect Favorite Color to have zero impact but its AUC is higher than even Gender, Smoke, Diabetes, Creatine Phosphokinase and Height.\n",
    "\n",
    "We could change the test size higher and also change the random_state seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_algo = 1\n",
    "f_testsize = 0.4\n",
    "i_seed = 3\n",
    "ls_features = ['Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is lower in this run. We can pretty much say it suggests no discrimimation.\n",
    "***\n",
    "We can try a combination of features next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_algo = 1\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "\n",
    "# identified least impactful features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Creatine phosphokinase', 'Height', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderately impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified most impactful features\n",
    "ls_features = ['Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderate and most impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin', 'Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# all features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6a. Experimentation and analysis of different algorithms on predictive ability\n",
    "We train and evaluate with Random Forest Classifier next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_algo = 2\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "\n",
    "# identified least impactful features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Creatine phosphokinase', 'Height', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderately impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified most impactful features\n",
    "ls_features = ['Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderate and most impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin', 'Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# all features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier model is excellent!\n",
    "***\n",
    "We train and evaluate with Support Vector Machine next. (this may take 6 to 7 minutes to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_algo = 3\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "\n",
    "# identified least impactful features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Creatine phosphokinase', 'Height', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderately impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified most impactful features\n",
    "ls_features = ['Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderate and most impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin', 'Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# all features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SVM model, the performance increases significantly with more features.\n",
    "***\n",
    "We train and evaluate with K-Nearest Neighbour next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_algo = 4\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "\n",
    "# identified least impactful features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Creatine phosphokinase', 'Height', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderately impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified most impactful features\n",
    "ls_features = ['Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# identified moderate and most impactful features\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin', 'Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)\n",
    "\n",
    "# all features\n",
    "ls_features = ['Gender', 'Smoke', 'Diabetes', 'Age', 'Ejection Fraction', 'Sodium', 'Creatinine', 'Platelets', 'Creatine phosphokinase', 'Blood Pressure', 'Hemoglobin', 'Height', 'Weight', 'Favorite color']\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], i_algo, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the KNN model, the performance increases significantly with more features. The jump in predictive ability with just the earlier identified moderate features is more than with the SVM model.\n",
    "***\n",
    "Let's compare the performance contributed by the moderate features across the 4 algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identified moderately impactful features\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin']\n",
    "\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 1, f_testsize, i_seed, True, False)\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 2, f_testsize, i_seed, True, False)\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 3, f_testsize, i_seed, True, False)\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 4, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see that the model trained on the Random Forest Classifier algorithm performs the best.\n",
    "***\n",
    "For completeness, let's compare the performance contributed by the most impactful features across the 4 algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identified most impactful features\n",
    "f_testsize = 0.3\n",
    "i_seed = 42\n",
    "ls_features = ['Ejection Fraction', 'Platelets', 'Blood Pressure', 'Hemoglobin', 'Age', 'Sodium', 'Creatinine', 'Weight']\n",
    "\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 1, f_testsize, i_seed, True, False)\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 2, f_testsize, i_seed, True, False)\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 3, f_testsize, i_seed, True, False)\n",
    "fig = pipeline_classifier(ls_features, df[ls_features], df['Survive'], 4, f_testsize, i_seed, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 7. Conclusion\n",
    "\n",
    "We see that the model trained on Random Forest Classifier being the best predictor followed by K-Nearest Neighbour.\n",
    "\n",
    "I reckon by virtue of its ability to correct for overfitting helps it outperform the other 3 algorithms tested.\n",
    "\n",
    "Further testing should be performed with unseen data to further increase confidence in the model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5feb96353fd922d5c54f8116c520f43fed13279f2fb12f561e18333e64d5b598"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
